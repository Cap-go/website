---
title: "@capgo/capacitor-llm"
description: Run LLM models locally in iOS and Android with native Apple Intelligence support for privacy and offline capabilities.
tableOfContents: false
next: false
prev: false
sidebar:
  order: 1
  label: "Introduction"
hero:
  tagline: Run LLM models directly on device with native Apple Intelligence integration and hardware acceleration.
  image:
    file: ~public/icons/plugins/llm.svg
  actions:
    - text: Get started
      link: /docs/plugins/llm/getting-started/
      icon: right-arrow
      variant: primary
    - text: Github
      link: https://github.com/Cap-go/capacitor-llm/
      icon: external
      variant: minimal
---

import { Card, CardGrid } from '@astrojs/starlight/components';

<CardGrid stagger>
  <Card title="On-Device Privacy" icon="approve-check-circle">
    Run LLM models directly on device for privacy and offline capabilities ğŸ¤–
  </Card>
  <Card title="Apple Intelligence" icon="star">
    Native Apple Intelligence integration on iOS 18.0+ ğŸ
  </Card>
  <Card title="Multiple Formats" icon="puzzle">
    Support for .gguf, .task, and .litertlm model formats ğŸ“¦
  </Card>
  <Card title="Hardware Acceleration" icon="rocket">
    Fast inference with native hardware acceleration âš¡
  </Card>
  <Card title="Streaming Responses" icon="download">
    Real-time streaming responses with event listeners ğŸ”„
  </Card>
  <Card title="Getting Started" icon="open-book">
    Check the [Getting Started Guide](/docs/plugins/llm/getting-started/) to install and configure the plugin.
  </Card>
</CardGrid>
