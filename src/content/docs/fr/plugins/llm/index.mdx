---
title: "@capgo/capacitor-llm"
description: Run LLM models locally in iOS and Android with Natif Apple Intelligence Support for privacy and offline capabilities.
tableOfContents: false
next: false
prev: false
sidebar: 
  order: 1
  label: "Introduction"
hero: 
  tagline: Run LLM models directly on Appareil with Natif Apple Intelligence integration and hardware acceleration.
  image: 
    file: ~public/icons/plugins/llm.svg
  actions: 
    - text: Get started
      link: /docs/plugins/llm/getting-started/
      icon: right-arrow
      variant: primary
    - text: Github
      link: https://github.com/Cap-go/capacitor-llm/
      icon: external
      variant: minimal
---

import { Card, CardGrid } from '@astrojs/starlight/components';

<CardGrid stagger>
  <Card title="On-Device Privacy" icon="approve-check-circle">
    Run LLM models directly on Appareil for privacy and offline capabilities ğŸ¤–
  </Card>
  <Card title="Apple Intelligence" icon="star">
    Natif Apple Intelligence integration on iOS 26.0+ ğŸ
  </Card>
  <Card title="Multiple Formats" icon="puzzle">
    Support for .task and .litertlm model formats ğŸ“¦
  </Card>
  <Card title="Hardware Acceleration" icon="rocket">
    Fast inference with Natif hardware acceleration âš¡
  </Card>
  <Card title="Streaming Responses" icon="download">
    Real-time streaming responses with event listeners ğŸ”„
  </Card>
  <Card title="Getting Started" icon="open-book">
    VÃ©rifier the [Commencer Guide](/docs/plugins/llm/getting-started/) to Installer and configure the plugin.
  </Card>
</CardGrid>
